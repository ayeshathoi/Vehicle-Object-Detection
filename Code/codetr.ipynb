{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pycocotools\n",
    "! pip install openmim\n",
    "! pip install mmengine\n",
    "! pip install addict\n",
    "! pip install yapf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U openmim\n",
    "! mim install mmcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git clone https://github.com/open-mmlab/mmdetection.git\n",
    "%cd mmdetection\n",
    "! pip install -v -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from decimal import Decimal\n",
    "import csv\n",
    "import random\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import tqdm \n",
    "import shutil\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.utils import get_git_hash\n",
    "from mmengine.utils.dl_utils import collect_env as collect_base_env\n",
    "\n",
    "import mmdet\n",
    "\n",
    "\n",
    "def collect_env():\n",
    "    \"\"\"Collect the information of the running environments.\"\"\"\n",
    "    env_info = collect_base_env()\n",
    "    env_info['MMDetection'] = f'{mmdet.__version__}+{get_git_hash()[:7]}'\n",
    "    return env_info\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for name, val in collect_env().items():\n",
    "        print(f'{name}: {val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pylabel \n",
    "from pylabel import importer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # path_to_annotations = \"/kaggle/working/dlenigma1/BadODD/labels/train\"\n",
    "# # path_to_annotations = \"/media/quadro/NVME/Asif-Thesis/MViT/custom_dataset/badodd/labels/train\"\n",
    "# path_to_annotations = \"/media/quadro/NVME/Asif-Thesis/MViT/custom_dataset/badodd/labels/val\"\n",
    "# path_to_annotations = \"/media/quadro/NVME/Asif-Thesis/MViT/custom_dataset/badodd/labels/test\"\n",
    "path_to_annotations = \"/media/quadro/NVME/Asif-Thesis/Enigma/processed_dataset/dhakaai/labels/test\"\n",
    "\n",
    "\n",
    "# #Identify the path to get from the annotations to the images\n",
    "# # path_to_images = \"/kaggle/working/dlenigma1/BadODD/images/train\"\n",
    "# # path_to_images = \"/media/quadro/NVME/Asif-Thesis/MViT/custom_dataset/badodd/images/train\"\n",
    "# path_to_images = \"/media/quadro/NVME/Asif-Thesis/MViT/custom_dataset/badodd/images/val\"\n",
    "path_to_images = \"/media/quadro/NVME/Asif-Thesis/Enigma/processed_dataset/dhakaai/images/test\"\n",
    "\n",
    "# #Import the dataset into the pylable schema\n",
    "# #Class names are defined here https://github.com/ultralytics/yolov5/blob/master/data/coco128.yaml\n",
    "yoloclasses = ['auto_rickshaw', 'bicycle', 'bus', 'car', 'cart_vehicle', 'construction_vehicle', 'motorbike', 'person', 'priority_vehicle', 'three_wheeler', 'truck', 'wheelchair']\n",
    "dataset = importer.ImportYoloV5(path=path_to_annotations, path_to_images=path_to_images, cat_names=yoloclasses,\n",
    "    img_ext=\"jpg\")\n",
    "\n",
    "dataset.df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset.export.ExportToCoco(cat_id_index=1, output_path='/media/quadro/NVME/Asif-Thesis/MViT/custom_dataset/badodd/images/train.json')\n",
    "# dataset.export.ExportToCoco(cat_id_index=1, output_path='/media/quadro/NVME/Asif-Thesis/MViT/custom_dataset/badodd/images/val.json')\n",
    "# dataset.export.ExportToCoco(cat_id_index=1, output_path='/media/quadro/NVME/Asif-Thesis/MViT/custom_dataset/badodd/images/test.json')\n",
    "dataset.export.ExportToCoco(cat_id_index=1, output_path='/media/quadro/NVME/Asif-Thesis/Enigma/processed_dataset/dhakaai/images/test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_vehicle = \"\"\"\n",
    "# Inherit and overwrite part of the config based on this config\n",
    "_base_ = ['co_dino_5scale_r50_8xb2_1x_coco.py']\n",
    "\n",
    "pretrained = 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth'  # noqa\n",
    "load_from = 'https://download.openmmlab.com/mmdetection/v3.0/codetr/co_dino_5scale_swin_large_16e_o365tococo-614254c9.pth'  # noqa\n",
    "\n",
    "train_batch_size_per_gpu = 1\n",
    "train_num_workers = 1\n",
    "# root directory where the data folder and annotation json are found\n",
    "data_root = '/media/quadro/NVME/Asif-Thesis/Enigma/processed_dataset/dhakaai/images/'\n",
    "max_epochs = 6\n",
    "stage2_num_epochs = 3\n",
    "base_lr = 0.00008\n",
    "\n",
    "# meta information about the number of classes and their annotation color in the inference output\n",
    "metainfo = {\n",
    "    'classes': ('auto_rickshaw','bicycle', 'bus','car', 'cart_vehicle','construction_vehicle','motorbike','person','priority_vehicle','three_wheeler', 'truck','wheelchair'),\n",
    "    'palette': [\n",
    "    (220, 20, 60),  # Red\n",
    "    (255, 165, 0),  # Orange\n",
    "    (255, 255, 0),  # Yellow\n",
    "    (0, 128, 0),    # Green\n",
    "    (0, 0, 255),    # Blue\n",
    "    (128, 0, 128),  # Purple\n",
    "    (255, 0, 255),  # Magenta\n",
    "    (128, 128, 128),# Gray\n",
    "    (255, 192, 203),# Pink\n",
    "    (0, 255, 255),  # Cyan\n",
    "    # (173, 216, 230),# Light Blue\n",
    "    (0, 255, 0),    # Lime Green\n",
    "    (139, 69, 19)   # Brown\n",
    "]\n",
    "}\n",
    "# model settings\n",
    "# number of classes must match the dataset's number of classes\n",
    "model = dict(\n",
    "    backbone=dict(\n",
    "        _delete_=True,\n",
    "        type='SwinTransformer',\n",
    "        pretrain_img_size=384,\n",
    "        embed_dims=192,\n",
    "        depths=[2, 2, 18, 2],\n",
    "        num_heads=[6, 12, 24, 48],\n",
    "        window_size=12,\n",
    "        mlp_ratio=4,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.,\n",
    "        attn_drop_rate=0.,\n",
    "        drop_path_rate=0.3,\n",
    "        patch_norm=True,\n",
    "        out_indices=(0, 1, 2, 3),\n",
    "        # Please only add indices that would be used\n",
    "        # in FPN, otherwise some parameter will not be used\n",
    "        with_cp=True,\n",
    "        convert_weights=True,\n",
    "        init_cfg=dict(type='Pretrained', checkpoint=pretrained)),\n",
    "    neck=dict(in_channels=[192, 384, 768, 1536]),\n",
    "    query_head=dict(\n",
    "        num_classes=12,dn_cfg=dict(box_noise_scale=0.4, group_cfg=dict(num_dn_queries=500)),\n",
    "        transformer=dict(encoder=dict(with_cp=6))))\n",
    "\n",
    "train_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='LoadAnnotations', with_bbox=True),\n",
    "    dict(type='RandomFlip', prob=0.5),\n",
    "    dict(\n",
    "        type='RandomChoice',\n",
    "        transforms=[\n",
    "            [\n",
    "                dict(\n",
    "                    type='RandomChoiceResize',\n",
    "                    scales=[(480, 2048), (512, 2048), (544, 2048), (576, 2048),\n",
    "                            (608, 2048), (640, 2048), (672, 2048), (704, 2048),\n",
    "                            (736, 2048), (768, 2048), (800, 2048), (832, 2048),\n",
    "                            (864, 2048), (896, 2048), (928, 2048), (960, 2048),\n",
    "                            (992, 2048), (1024, 2048), (1056, 2048),\n",
    "                            (1088, 2048), (1120, 2048), (1152, 2048),\n",
    "                            (1184, 2048), (1216, 2048), (1248, 2048),\n",
    "                            (1280, 2048), (1312, 2048), (1344, 2048),\n",
    "                            (1376, 2048), (1408, 2048), (1440, 2048),\n",
    "                            (1472, 2048), (1504, 2048), (1536, 2048)],\n",
    "                    keep_ratio=True)\n",
    "            ],\n",
    "            [\n",
    "                dict(\n",
    "                    type='RandomChoiceResize',\n",
    "                    # The radio of all image in train dataset < 7\n",
    "                    # follow the original implement\n",
    "                    scales=[(400, 4200), (500, 4200), (600, 4200)],\n",
    "                    keep_ratio=True),\n",
    "                dict(\n",
    "                    type='RandomCrop',\n",
    "                    crop_type='absolute_range',\n",
    "                    crop_size=(384, 600),\n",
    "                    allow_negative_crop=True),\n",
    "                dict(\n",
    "                    type='RandomChoiceResize',\n",
    "                    scales=[(480, 2048), (512, 2048), (544, 2048), (576, 2048),\n",
    "                            (608, 2048), (640, 2048), (672, 2048), (704, 2048),\n",
    "                            (736, 2048), (768, 2048), (800, 2048), (832, 2048),\n",
    "                            (864, 2048), (896, 2048), (928, 2048), (960, 2048),\n",
    "                            (992, 2048), (1024, 2048), (1056, 2048),\n",
    "                            (1088, 2048), (1120, 2048), (1152, 2048),\n",
    "                            (1184, 2048), (1216, 2048), (1248, 2048),\n",
    "                            (1280, 2048), (1312, 2048), (1344, 2048),\n",
    "                            (1376, 2048), (1408, 2048), (1440, 2048),\n",
    "                            (1472, 2048), (1504, 2048), (1536, 2048)],\n",
    "                    keep_ratio=True)\n",
    "            ]\n",
    "        ]),\n",
    "    dict(type='PackDetInputs')\n",
    "]\n",
    "\n",
    "train_dataloader = dict(\n",
    "    batch_size=train_batch_size_per_gpu,\n",
    "    num_workers=train_num_workers,\n",
    "    dataset=dict(\n",
    "        data_root=data_root,\n",
    "        metainfo=metainfo,\n",
    "        data_prefix=dict(img='train/'),\n",
    "        ann_file='train.json'))\n",
    "print(train_dataloader);\n",
    "\n",
    "test_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='Resize', scale=(2048, 1280), keep_ratio=True),\n",
    "    dict(type='LoadAnnotations', with_bbox=True),\n",
    "    dict(\n",
    "        type='PackDetInputs',\n",
    "        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n",
    "                   'scale_factor'))\n",
    "]\n",
    "\n",
    "# This notebook uses the train data as validation. If you want to use a separate validation dataset, \n",
    "# you must put the paths for validation instead train in the val_dataloader\n",
    "val_dataloader = dict(\n",
    "    dataset=dict(\n",
    "        data_root=data_root,\n",
    "        metainfo=metainfo,\n",
    "        data_prefix=dict(img='test/'),\n",
    "        ann_file='test.json'))\n",
    "test_dataloader = val_dataloader\n",
    "\n",
    "# Modify metric related settings\n",
    "# for validation set, change the train.json to the name of the validation json.\n",
    "val_evaluator = dict(ann_file=data_root + 'test.json')\n",
    "test_evaluator = val_evaluator\n",
    "\n",
    "optim_wrapper = dict(optimizer=dict(lr=1e-4))\n",
    "\n",
    "\n",
    "train_cfg = dict(max_epochs=max_epochs)\n",
    "# out_dir is the directory where the checkpoints are saved\n",
    "default_hooks = dict(checkpoint=dict(type='CheckpointHook', save_best='auto',out_dir='../checkpoints_codetr_final'))\n",
    "param_scheduler = [\n",
    "    dict(\n",
    "        type='MultiStepLR',\n",
    "        begin=0,\n",
    "        end=max_epochs,\n",
    "        by_epoch=True,\n",
    "        milestones=[8],\n",
    "        gamma=0.1)\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open('./projects/CO-DETR/configs/codino/co_dino_5scale_swin_l_16xb1_16e_o365tococo.py', 'w') as f:\n",
    "    f.write(config_vehicle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python tools/train.py ./projects/CO-DETR/configs/codino/co_dino_5scale_swin_l_16xb1_16e_o365tococo.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tools/test.py ./projects/CO-DETR/configs/codino/co_dino_5scale_swin_l_16xb1_16e_o365tococo.py ../co_dino_5scale_swin_l_16xb1_16e_o365tococo/epoch_1.pth --out results.pkl --work-dir ./results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !bash tools/dist_train.sh ./projects/CO-DETR/configs/codino/co_dino_5scale_swin_l_16xb1_16e_o365tococo.py 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
